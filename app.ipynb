{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "\n",
    "from langdetect import detect_langs\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #twitter api keys\n",
    "# auth = tweepy.OAuth1UserHandler(\n",
    "#    )\n",
    "\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_words = \"#wakandaforever OR #WakandaForever OR #blackpanther OR #WakandaForeverID OR #BlackPantherWakandaForever OR #BlackPanther OR #BlackPanther2\"\n",
    "# search_query = search_words + \" -filter:retweets AND -filter:replies\"\n",
    "\n",
    "\n",
    "# limit = 100000\n",
    "# tweets = tweepy.Cursor(api.search_tweets, q=search_query, tweet_mode='extended').items(limit)\n",
    "\n",
    "# data=[]\n",
    "# for tweet in tweets:\n",
    "#     data.append([tweet.id,tweet.user.screen_name, tweet.created_at, tweet.full_text,tweet.user.location, tweet.retweet_count,tweet.favorite_count, tweet.user.followers_count, tweet.user.friends_count, tweet.user.verified, tweet.source])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns =['id','username','time_of_tweet', 'tweet', 'location', 'retweets', 'likes', 'followers', 'following', 'verified', 'tweet_source']\n",
    "# bp_tweets=pd.DataFrame(data, columns=columns)\n",
    "# bp_tweets.to_csv(\"BlackPanther-24-11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save scrapped data to csv \n",
    "tweets = pd.read_csv('BlackPanther-24-11.csv')\n",
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column names and datatype\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical description of the dataset\n",
    "tweets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnamed column\n",
    "tweets.drop(columns='Unnamed: 0', inplace = True)\n",
    "\n",
    "#fill in missing location with 'None'\n",
    "tweets['location'].fillna('None', inplace=True)\n",
    "\n",
    "#change the datatype of tweet_time to datetime \n",
    "tweets['time_of_tweet'] = tweets['time_of_tweet'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash(tweet):\n",
    "    '''This function collect all the hashtags in the tweet column'''\n",
    "    tweet = re.findall(r'\\#\\w+', tweet)\n",
    "    return \" \".join(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the get_hash function\n",
    "tweets['hashtags'] = tweets['tweet'].apply(get_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurrence of each hashtags in the hashtags columns\n",
    "hashtags = pd.Series(\" \".join(tweets['hashtags']).split()).value_counts()\n",
    "#hashtags.to_csv('hashtag_freq.csv')    ##save the output as a csv\n",
    "hashtags[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_languages(tweet):\n",
    "    '''This function detects the language in each \n",
    "    tweet and returns it in ISO 639-1 codes'''\n",
    "    try:\n",
    "        tweet = detect_langs(tweet)\n",
    "    except: \n",
    "        tweet = \"nolang\"\n",
    "    tweet = str(tweet).split(':')[0][1:]\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the get_languages function\n",
    "tweets['language'] = tweets['tweet'].apply(get_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substituting the short form language names for the long names\n",
    "lang_code = pd.read_csv('language_codes.csv')\n",
    "dict_lang_code = dict(zip(lang_code['alpha2'], lang_code['English']))\n",
    "tweets['language'] = tweets['language'].replace(dict_lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for changes\n",
    "tweets.language.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(tweets):\n",
    "    '''This function iterate over the tweets and return \n",
    "    'Yes' is there's a link and 'No' if there's no link '''\n",
    "    link = re.findall(r'((http|ftp|https):\\/\\/)(([\\w.-]*)\\.([\\w]*))', tweets)\n",
    "    if len(link) < 1:\n",
    "        return 'No'\n",
    "    else: return 'Yes'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the get_links function\n",
    "tweets['contains_media'] = tweets['tweet'].apply(get_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for changes\n",
    "tweets.contains_media.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:3\u001b[1;36m\u001b[0m\n\u001b[1;33m    tweets = tweets.lower()\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "cast = ['namor', 'shuri', 'ironheart', 'ramonda', 'namora', \n",
    "        'okoye', 'aneka', 'nakia', 'mbaku', 'attuma', 'everett', ]\n",
    "\n",
    "\n",
    "def get_cast(tweets):\n",
    "        '''This function extracts names of the cast into a new column'''      \n",
    "        tweets = tweets.lower()\n",
    "        tokens = word_tokenize(tweets)\n",
    "        cast_names = [name for name in tokens if name in cast]\n",
    "        cast_names_title = [name.title() for name in cast_names]\n",
    "        return \" \".join(cast_names_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the get_cast function\n",
    "tweets['cast_name'] = tweets['tweet'].apply(get_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurrence of each name in the cast_name columns\n",
    "names = pd.Series(\" \".join(tweets['cast_name']).split()).value_counts()\n",
    "# names.to_csv('names_freq.csv') ##save the generated dataframe\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the '\\n' characters with space\n",
    "tweets['tweet'] = tweets['tweet'].replace('\\n', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the sentiment scores using Sentiment Intensity Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "tweets['sentient_scores'] = tweets['tweet'].apply(sia.polarity_scores)\n",
    "tweets['sentient_scores'] = tweets['sentient_scores'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweet):\n",
    "    '''This functions returns Positive, Negative or \n",
    "    Neutral depending on the state of the sentiment score'''\n",
    "    if tweet > 0:\n",
    "        return \"Positive\"\n",
    "    elif tweet == 0:\n",
    "        return \"Neutral\"\n",
    "    else: return \"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the get_sentiment function\n",
    "tweets['sentiment'] = tweets['sentient_scores'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for changes\n",
    "tweets['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(inc_c, case, cor_c):\n",
    "    '''This function substitutes the current text for the '''\n",
    "    tweets.loc[tweets['c_location'].str.contains(inc_c, case=case), 'c_location'] = cor_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['c_location'] = tweets['location']\n",
    "\n",
    "\n",
    "dicts = {'brazil':'Brazil', 'brasil':'Brazil', 'nigeria':'Nigeria', 'méxico': 'Mexico', 'france':'France', 'india':'India', 'london':'United Kingdom', 'UK':'United Kingdom', 'england':'United Kingdom','Bonaire':'Bonaire Islands', 'España':'Spain', 'thailand':'Thailand', 'Ohio':'United States', 'South Africa':'South Africa'}\n",
    "for key, val in dicts.items():\n",
    "    get_country(key, False, val)\n",
    "\n",
    "\n",
    "list_us = ['USA', 'KS', 'FL', 'CA', 'GA', 'AZ', 'CO','IL', 'TX', 'PA', 'DC', 'TN', 'NY', 'WA', 'NYC', 'NC', 'NV', 'OH', 'LA', 'MA', 'Los Angeles' 'new york','New York', 'Los Angeles']\n",
    "for states in list_us:\n",
    "    get_country(states, True, 'United States')\n",
    "\n",
    "tweets['c_location'].replace('None', np.nan, inplace=True)\n",
    "tweets['c_location'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tweet_source'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['verified'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('BlackPanther_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
